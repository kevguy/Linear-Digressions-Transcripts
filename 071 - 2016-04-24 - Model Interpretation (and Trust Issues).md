| Person | Transcripts |
| :-- | :-- |
| Ben: | Hey Katie. |
| Katie: | Hi Ben. |
| Ben: | So today we're going to talk about black boxes? |
| Katie: | Yes, data collection devices on airplanes. No... no… |
| Ben: | No. |
| Katie: | No, although it’ll be kind of interesting, but no. Today we're going to talk about machine learning models and one of the most frequent and legitimate complaints about them is that they are black boxes. You put something in and then something comes out, and you have no idea what happens on the inside, and… and this is a real... this is a real thing and we're going to talk about some work that people have done to try to open up the black box little bit, make models more interpretable. |
| Ben: | Awesome, you are listening to the Linear Digressions. |
| Ben: | Okay, so before we actually dive into the machine learning part there, let’s kind of take a step back and talk about what is a black box. So you kind of give a quick summary... but a black box is basically something where you put something in and something comes out, but you have no idea what happens in the... in the middle space. So this could be a machine learning algorithm that we're going to be talking about today. Actually one of my favorite black boxes is the human brain. We have no idea how the human brain works, even though every one of us has one. We just know that if you give it certain kinds of input, it will give certain kinds of output. For example, if you keep poking somebody, then that brain is probably going to get pissed off, and get angry right? And then exhibit certain kinds of output, like maybe a flushing face or punching someone, I don't know. |
| Katie: | Yeah. |
| Ben: | That's kind of a silly example but fundamentally… |
| Katie: | No, it's not a silly example. I think it was actually kind of... a... that was an interesting choice of… of… cases to talk about. Because, of all of the black box machine learning models, neural nets, in my opinion, are among the black box, it's like you know... you know what you are sending and... and you know what comes out, but trying to understand what's going on on the inside, either through math or through visualisations or any of these things, can be really really hard, but because of the structure of the algorithm itself and also because of the complexity and just things get all mixed together, and it's... yeah… |
| Ben: | And neural nets are based on human brains, the way they work. |
| Katie: | The original inspiration of the structure of the algorithm yeah, it’s from human brains. So there's something about... something about brains that is hard to really understand. |
| Ben: | Brains are hard. I don't understand mine. |
| Katie: | Me either. |
| Ben: | So... let's go into machine learning world. |
| Katie: | Sure, yeah. So one of the things that maybe you've had this problem if you work with machine learning models or it's a question that you might have had is... let’s suppose you have a machine learning model and it can be... I’m kind of model agnostic when I... when I talk about this, it can be a linear model, it can be a decision tree, it can be a neural net, it maybe SVM, like anything. You have this model and you send things into it, you send features of some case that you wanna classify… let's say we'll take the case of classification. So sending the features and then it spits out a label. And then you want to understand why it gave you this label, which is an extremely reasonable question to ask. Cuz like okay, I know you're telling me that this is a picture of… let’s take the case of of dogs and wolves actually, because this was relevant for the paper that we're talking about, oh, which I should totally say what it is. We're talking about a paper called “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier” This comes from a trio of researchers at the University of Washington, and there's also a pretty good software package that's on GitHub that actually implements the algorithm here. So… |
| Ben: | I love that name. |
| Katie: | “Why should I trust you?”? |
| Ben: | Yeah. |
| Katie: | Yeah, the name of the algorithm that they actually wrote is called the LIME. L-I-M-E, fo… hat does it stand for… Locally Interpretable Model Agnostic Explanations. |
| Ben: | Okay, so they actually had a reason to name it LIME. I mean… |
| Katie: | Yeah. |
| Ben: | A lot of... a lot of high-tech things these days are just named after things like fruits and… |
| Katie: | Yeah, no. It's not the most gimmicky name that I've ever seen. It's got a little bit… this is a little bit tailored to reality, but they have this good case in here at some point that they're building a classifier that was... that was classifying pictures of Huskies, like the dog, from pictures of wolves. So we'll use that example, you have some black box classifier, and you send in a picture, it sort of takes all the pixels, or the patches, patches are like the contiguous pixels that are very similar. |
| Ben: | Ah okay, like pixel regions. |
| Katie: | Exactly, yeah. So it takes sort of that input information, something happens under the hood, and then it spits out we think this is a dog or we think this is a Husky. And you know, good luck if you want to try to explain why it’s… |
| Ben: | Yeah, that piece in the middle is like… magic happens. |
| Katie: | Right, and usually... usually we hope that the magic is happening in... in a good way, and we have things like accuracy or various kinds of performance metrics that can help us understand how well the magic under the hood might be doing its job, but… |
| Ben: | But there you're just measuring the output? |
| Katie: | Yeah. and there can be cases where accuracy or other kinds of metrics can fool you. And in fact, they... they give this as one of the motivations for doing this work in the first place is that accuracy is... is a number that can be kind of like hacked in... in certain ways if things line up just wrong and so you want to be a little bit careful and... it's great if you can have something that is a little bit more interpretable to human and that will help you avoid some of those like pitfalls of just leaning on a number. |
| Ben: | So can you give me an example of... kind of the way that the stars could align. I mean, so we've got this model that's trained on a dataset of pictures of huskies and pictures of wolves and is trying to decide which is which. |
| Katie: | Yeah, so the reason I came up with huskies and wolves is they actually engineered this example to be... to be a little bit… to be a little bit pathological. What they did was they trained on pictures where the husky it is in front of a background that has snow in it and the wolf does not. |
| Ben: | Oh, okay. |
| Katie: | And that… |
| Ben: | Because Huskies... I don't know... do Huskies live more in snowy areas? |
| Katie: | Yeah, they live in Alaska usually… |
| Ben: | Right, so you can imagine like this could be… |
| Katie: | But there’s a little bit Chicago. So yeah, it’s a little bit… I mean they sort of artificially made… |
| Ben: | So somewhat contrived. |
| Katie: | Well… the thing is it’s not. There’s actually a very famous case from the early days of image recognition where they had an algorithm… this is being done by the Department of Defense or something. And they wanted to have an image recognition algorithm for figuring out if they were tanks in photographs. So I had all these photographs of tanks and not-tanks, fed into the algorithm. And then the algorithm got really really good at... you know basically the training dataset, but they only realized they generalized really poorly. They only realized later that all of the tank pictures were taken during the day and all of the not-tank pictures were taken at night. |
| Ben: | Wow, wow. |
| Katie: | So, basically it was just learning what day and night were, it had no idea what a tank was. |
| Ben: | So you're saying that this husky example... even though it was contrived for this particular purpose on this paper. It... first of all... it could be not contrived, like you could actually have a training dataset that happens to align that what... that way, or maybe there's actually a pretty strong correlation between something like this, like an animal that lives in the snow pretty exclusively and another animal you're comparing it to that doesn't really ever get photographed in front of snow. And so, the machine learning algorithm basically learns the wrong thing because of that correlation. |
| Katie: | Right, and so then, what you want to do in those cases is if you want to have a way of asking machine learning algorithm, basically like “hey why did you say that?”, and if it says something like “oh wow, look at this... look at the snow in the background.”, that would be a different kind of explanation than I see that the dog in this picture is standing on top of a moose carcass which probably means it's a wolf and not a husky or whatever, wolves and huskies actually look pretty similar so it's hard to think of… it’s hard to think of an exact... |
| Ben: | This is where the black box part comes in? |
| Katie: | Oh yeah, because black box is not going to be able to give you an answer to that question, but that is what these researchers are trying to unpack a little bit, see if there's something that can be done. The task here is to come up with what they call a good explanation, which has a few attributes of it. One is that it be interpretable for a human. Second is that it be locally faithful to how like this... we have the sort of... like simpler model, it's something that's interpretable to human. It's not beginning to be... it's not the same as the full super complex model, but in the area that we care about, it's close to the full complex models, so we call this “local fidelity”. And the same as it... being the same is a full model in... in other places, so we don't need it to extrapolate to other areas, that just needs to tell me so like... what's going on in the neighborhood that I care about. |
| Ben: | So you've got to... you basically have a similar version of the complex model that is representative within the area that you care about? |
| Katie: | Bingo, yeah. Third, is that... you want to be model agnostic, so ideally it would be something that you could send in any black box, and it would be able to give you explanations, it's not... for example, you don't have dependents on the structure of the model built into your explainer, that means that you can use it on other algorithms easily. |
| Ben: | So, big picture, the explainer… let's just call it the explainer, is actually a totally separate piece… it’s the thing that they're describing in this paper. |
| Katie: | Yes. |
| Ben: | You take the model of say... the Huskies and the wolves, and you want to be able to pass in this relatively complex model into the explainer... with maybe some information about what kinds of things that you care about or you're looking for and then explainer should be able to take this model, create a simpler model, and then figure out from that simpler model, what's actually going on. |
| Katie: | Yeah, that's exactly how it works. And then the fourth thing that they say is that ideally you would want something that is kind of extensible from single predictions to the full... the full model. So by sort of taking lots of representative samples of the kinds of data that we're going to be using this model to classify, we take those representative samples, we kind of map out the entire reasonable spacing which this model might be functioning. And if all of those predictions look like they're making sense, then we can say not only do we have confidence in the individual predictions that this model makes, but we have confidence in the model itself in a more certain general or global way. |
| Ben: | So that's exciting, because not only are you making a simple version based on this complex model and then getting... explanations within that relatively small space of what you care about, but you could potentially say, “I care about everything, why don't you do that with all of these different pieces and then maybe give me insight into what this complex model is doing by building a bunch of different simple versions that all care about different things?”. Does that... does that make sense? |
| Katie: | Yeah that's it exactly, and that's kind of the second half of this paper is it something called a SP-LIME, which is where... if you have LIME and LIME is an algorithm that looks for sort of the simple models that allow you to take sort of a subset of the features and decide which of those features are the most important, so this would be like the patches of pixels or the words. You say like... what are the words that are most important for the the prediction that you just made? And then we'll sample from your data in kind of a representative way so that you get an idea overall that what the model is... is doing something that's sensible. So that's pretty cool. |
| Ben: | Yeah, that's really neat. It's basically a way of breaking down the problem but the problem that you're breaking down is a problem that on its own is too complex to understand. |
| Katie: | Yeah, so you're solving it in like... a lot of individual places and then saying that if all of those places... it looks like things are okay, then we're moving toward the place where the model as a whole is okay. And so, then the way that they checked that SP-LIME is actually… for example, going to give you more helpful information about whether your model is doing something sensible or not. They took this this wolf-dog example is actually one of the ways that they tried to verify that the explanations of the models that people were getting were actually helping sort of these non-expert evaluators decide which of two models was… was better. So they sent to people on Mechanical Turk, so these are just non-experts… |
| Ben: | They’re just random people. |
| Katie: | Random people from the Internet. |
| Ben: | Wow. |
| Katie: | And they sent them some sort of descriptions and representative pictures from this neural net model that had been sort of purposely trained to recognize whether there's snow. And they asked people on the Internet like... do you think that this model would do a good job of classifying sort of arbitrary additional new pictures of wolves and dogs. And people were like sort of mixed on it, some of them said yeah, it looks like it would do a good job, some of them said no, but the idea is that the correct answer here is no, because what it's really doing is just figuring out what the background is. |
| Ben: | Right, but in order to answer no, you, as a human, need to identify that it's probably just looking at the background. |
| Katie: | Right, and so… |
| Ben: | Which means that you, as a human, need to... need to see the pattern that it's finding. |
| Katie: | And that's what LIME allows you to do, is if they sent a different group of subjects with the same information but now with this additional layer of explanations where it’s sort of highlighting the patches of pixels that are being identified as important for making the classification choices, and basically everyone said no, this is not going to do well in future cases because it's... it's… it hasn't actually learned what a dog or Husky is. It's just learn what snow looks like. |
| Ben: | It's really cool. |
| Katie: | Yeah, it was pretty neat. And then i think the the neatest thing about this is I found the paper, to be completely honest, I find it kind of hard to understand and some of the inner workings... I'm not sure I still completely understand, I'm not sure if I'll be able to, for example, to replicate this... this algorithm on my own just from the paper. However, I don't have to because there's a GitHub package that has LIME in it and SP-LIME and it has some IPython notebooks where they actually take a Scikit Learn classifiers and they train them up, things like random forest, things that are hard to understand. And then they send them into the LIME and then LIME will produce list of explanations and pictures and then you can actually see... sort of how certain documents are getting classified as... you know, if atheism vs. Christianity, I think it is... it is the example that they use. And so, that's kind of neat that the code is all available there with some examples and you can go in and try it out and see if it works for you, this is something that I don't see as often as I wish I did, and it's... it's really like... it's really pretty fun. So if you're ever in the market for some like... model interpretation, give it a try and let us know how it goes. I think that's neat.
