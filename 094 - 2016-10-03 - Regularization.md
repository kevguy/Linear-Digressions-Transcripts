| Person | Transcripts |
| :-- | :-- |
| Ben: | Hey Katie. |
| Katie: | Hi Ben. |
| Ben: | So we've had a lot of fun episodes recently, I'm just wondering if maybe we can have a normal episode. |
| Katie: | Like a regular episode? |
| Ben: | A regular episode, yeah. |
| Katie: | Ha ha. |
| Ben: | You’re listening to Linear Digressions. |
| Ben: | That was a bad pun, because we're going to talk about regularization, is that what it's called? |
| Katie: | Yeah that's what it’s called. Regularization is one of the things that I use everyday, oh, everything we talk about is something I use every day, regularization is one of those things. So a lot of times when you're doing something like machine learning, your statistics... there's usually kind of an assumption that baked into a lot of these algorithms when they were first being put together which is that, when you're thinking about the data that you have to train it, you have many more samples in that data than you have attributes of each sample, so another thing about that is if you have a table that has all your data in it and each one of the samples that you take, or each... each instance, each training case, is a row and then each attribute of that training case is a column, you have many more rows in the table then you have columns. |
| Ben: | Right, so that's like you do a sample of 3 attributes of people, and you sample 1000 people, that's 1000 rows and 3 columns? |
| Katie: | Yeah, and so for a lot of machine learning applications that's actually a really nice and statistical applications too. That’s really nice regime to be in because you have sort of a lot of chances to get familiar with the patterns in the data, and the patterns aren't so complicated that you don't really have enough data to like fully explore the space, like the data very... very much like gives you the full picture of what's going on, and... and you can start to learn the patterns in it right? |
| Ben: | Yeah, and so like if you look at… for example, the ages of your thousand people, you're very likely to get a nice normal distribution of ages, whereas you only have like ten or twenty or fifty rows, in other words, it’s 50 people, then your ages will be a lot less… I guess consistent with reality if you weight them normally right? Because you might get a lot more older people than younger people just by random statistical chance. |
| Katie: | Oh I see, yeah. So I hadn’t even been thinking about that, but it's yeah, there’re sort of 2 things in there and let me just tease it apart a little bit. So the thing that you were just talking about, if we only have 50 rows in our sample let’s say, then yeah, we're in a regime where it's very likely we're going to run into kind of statistical fluctuations, so in general when you have just not very much data to begin with, that's always going to be a tough place to live. |
| Ben: | Right. |
| Katie: | And what I'm talking about is something that's slightly different, although sort of related, I'm talking about the scenario where you can have lots and lots of data perhaps, but it's more about the ratio between the number of rows and the number of columns. So even if you have let's say a thousand samples, if each one of those samples has a million features then you're still in a regime here even though you have... you have a thousand sample, you know, that's a lot of data, well, it’s sort of a lot of data, but it's definitely much, a thousand is much less than a million, as you’re well aware, so even though we have a thousand in and of itself, if you had three columns or something, a thousand might be just fine, but the fact that you have a million columns instead of a thousand, it is not fine. |
| Ben: | So I guess you're going to talking almost like, so I work with graphics, and so I am almost thinking of this as the aspect ratio of your data, if rows is the height, columns as the width, if your data is a lot wider than it is tall, you're going to have to interact with it differently, very differently, then if it's more tall than it is wide. |
| Katie: | Yeah, yeah, and I think that even if you have plenty of rows even, working in space is where you have higher dimensionality can just be tough, even if you have let’s say a million examples that you can learn from and 1000 columns for flipping things around now, and it seems like we should be very happy. If you're going to try to build a model with let’s say, you’re building a regression or something, a simple linear model with that, that means that there's potentially 1000 different features that you think can affect your outcome, and that's kind of a lot to keep track of, let’s imagine that you were trying to explain to your boss if you built this model, what does this model do, you’ll have to tell him a thousand pieces of information, or her, tell her 1000 piece of information, before she had the full picture of what this model is trying to tell you and it's just kind of a lot of things to keep track of. So… |
| Ben: | Right. It’s also just, I mean, even if you can get the computer to kinda grind all this stuff, it's really hard to understand it as a human and to convert whatever your model is doing into... I don't know, like a dissertation or a report to your clients, or whatever it is. |
| Katie: | Yeah, so depending on the regime, there’s still might be something that is perfectly valid, and in general, you know, this is not a hard-and-fast rule that there's any maximum number of features you should be thinking about, but yeah if you want to try to interpret this or if you want to tell some kind of story, especially if you're trying to model something that should be kind of intuitive, and you want it to be graspable in a way, that you know, not everything always is, but sometimes, we want it to be something that is understandable and usually a thousand features is not a particularly understandable place to be, right? |
| Ben: | Right. |
| Katie: | So okay, so working with high dimensional data, it’s just, it’s got some tricks. |
| Ben: | Oh, and so you're, so we’re talking about regularization which I'm guessing is taking that high dimensionality, the large number of columns, and trying to find a way to minimize that. |
| Katie: | Yeah, so regularization is a fairly specific way of doing that to dimensionality reduction, there are several different big classes of dimensionality reduction that you can think of, depending on exactly what you think about, perhaps like the internal structure of the data, and how you want a privilege certain types of data relative to other types of data. But yeah, regularization is sort of one of the... one of the really big ones, one of the things that makes a lot of sense to just sort of throw regularization at it and let it sort it out. So regularization, there's a number of different names for the different kinds of regularization algorithms, see you might hear this being referred to as (???), or Generalized Linear Models which is actually a bigger set of models than just regularization but sometimes people refer to those things kind of in the same way, you may hear LASSO Regression, or Ridge Regression, or Elastic Net Regression, all types of regular session, but in general what regularization is going to do is if you're running it on regularized model, this is sort of something that you're going to run just out of the box before you’re trying to do any dimensionality reduction, what that model's going to do in the course of fitting is... is trying to come up with a formula that will minimize the error on the predictions that you make, so it is trying to find equation that's going to predict each one of your cases with as small of an error as possible. |
| Ben: | Mm hmm. |
| Katie: | And what regularization is going to do is it says, ‘we want you to simultaneously take into account how big the errors of your model but also the complexity of the model and the complexity of model basically how many of the... of the features are you actually using in the model.’ |
| Ben: | Got it, so rather than just saying ‘hey, you know, do your best to minimize the error’, you’re saying ‘do your best to minimize the error but also look at the complexity, you don't want to have crazy complexity with very little error, you might actually be willing to accept a little bit more error to have significantly reduced complexity.’ |
| Katie: | Yeah, and so exactly the way that it trades off between the goodness of fit and the complexity, that's what makes different kinds of regularizations different from each other, LASSO, Elastic Net, blah blah blah. But in general the thing that they have in common is there’s such thing called the regularization perimeter and that basically trades off between how important it is for you to have a model that fits the data very closely versus a model that's simpler, and so the stronger the regularization parameter is that basically penalizes you for adding more complexity of the models, you're still allowed to add more variables but they just better be really darn good, they’d better really help you on the goodness of fit of the model, in order for it to sort of overcome the penalty that you pay for adding it in. |
| Ben: | Right, that makes sense, so it's just kind of trying to do that cost benefit analysis in a good way? |
| Katie: | Exactly, and so what these regularized models are going to do, each of them is a little bit different in the details of exactly what it does, but they'll select for you... I’ll pick LASSO Regression cause this is... this is the one that I use the most. LASSO Regression will actually... just entirely throw out parameters but don't pass the regularization scheme, so you can take, let’s say a thousand features, dump it into a LASSO Regression and depending on exactly with the strength the regularization parameter is, it might only send back a model that has ten or twenty or fifty or a hundred variables in it. So it's actually throwing out a whole lot of the data. |
| Ben: | Wow, yeah, throwing out like ninety to ninety-nine percent of it in a lot of cases? |
| Katie: | Yeah, yeah, and so… again… |
| Ben: | I mean, yeah, if it doesn't actually strongly change your path then it doesn't matter. |
| Katie: | Right, right. So it’s saying, you're allowed to have more data, just has it sort of earn its place in the model, if it’s not making the model, better by a big enough margin that it’s not gonna get selected. And it's kind of interesting, just a little background about LASSO. So the place where this becomes really important in the real world to the best of my knowledge is in genomics, because each person... if you're trying to do genomic analysis, let’s say you're trying to study correlations between the genes that you see in someone's genome and various types of proteomics or symptoms that they have of some specific disease or whatever, there's potentially many many genes that can be responsible for the phenomenon, that it is that, you're studying and you don't necessarily know which ones are the important ones, so you want to be open the possibility of any of them. So you want to have this very high dimensional inputs space, because you have all these genes. |
| Ben: | Right, of course, and it's also difficult to get this information, you’re not going to have a million people, you're going to have lower, I guess, number of rows. |
| Katie: | Right, yes if you think about the context in which these studies usually happen, it's a small research team, that's maybe working in one hospital, or one university and so, they only have the ability to collect maybe 100 genomes or something like that, and there's potentially millions of different genes they’re studying. So they're very much in this regime where there's many more columns in their data than there is rows. And so in general, there's all kinds of things that you have to be careful about when you're working with this data, regularization is just one of them and I hope that in some future episode we will pick up some of the complexities of things like hypothesis testing when you have very high dimensional features space, but for just tackling problem of what should we even be paying attention to, regularization is going to be one of your best friends. |
| Ben: | Okay, Katie, what were those normalization algorithms you were talking about again? |
| Katie: | Oh, regularization, you mean? |
| Ben: | Oh, excuse me. |
| Katie: | No, it’s fair enough, I used to get those two mixed up all the time honestly when I was first learning this, so… |
| Ben: | Actually, this kind of illustrates my point. I just remember you mentioned so many of them and I was thinking ‘Gosh, could I be a data scientist knowing that I constantly get words confused and I constantly get the name... the names of things confused, and I guess it's just that it's not my field, and in my field I'm a little bit better, but I mean what is that... what has that journey been like?’ |
| Katie: | Oh yeah, that’s a good question. So the first thing that I should say is that when I’m doing this podcast I pick the things that I feel comfortable with to talk about, so you're getting a biased sample of that areas of my expertise. |
| Ben: | Oh I see. |
| Ben: | So that’s the first secret is... |
| Katie: | Cause you always sound really smart. |
| Ben: | I picked topics somewhat carefully. |
| Katie: | Right. |
| Ben: | And yeah, it's a really good point because there is a lot of jargon and obviously the more familiar you get with this field… like any field you’ll just become familiar with it, and you get to know it and it's not... it's not as scary anymore, once you know what's working under the covers. But if I'm being completely honest, so I was talking about a number of different regularization schemes, I talked about LASSO, I talked about Ridge, I talked about Elastic Net, we didn’t talk about the differences, but I just… you know, kind of ticking off my fingers. And there's differences and exactly how they calculate what complexity means, and this has to do with technical details or something called like an L1 norm and L2 Norm of that has to do with the size of the coefficients of the variable that you're putting in the model, and all this is to say that L1 and L2 I think are very undescriptive names, and I didn't know that... it’s not like I didn't know the difference of them, but one of them is LASSO and one of them is Ridge and… |
| Katie: | Oh interesting. |
| Ben: | I'm honestly have like a little cheat sheet that sometimes I have to refer to to remember which one is which cause you know, when you use them a whole lot, I guess, now I know the difference cause I use LASSO so much, I just, you know… |
| Katie: | Yeah, you’ll just memorize it. |
| Ben: | Yeah, you’ll just kind of like memorize it, but for the longest time when I was getting used to regularization and I knew that they both existed but I just couldn't remember which one was which and it made me feel kind of... kind of dumb because everybody is like taking the stuff off like it ain't no thing, but... but yeah, it's... that's an example of something that you know, I had to do this every day for six months before I was able to kind of fake my way through it adequately. |
| Katie: | Right, yeah, and actually thinking about it now, the same goes for me, so I'm a front-end web developer most of the time at least, my professional career. So I write a lot of JavaScript, and you know, earlier on, if you ask me ‘Alright, what's the difference between array.shift() and array.unshift()?’ I know that one of them put something at the beginning of an array and one of them took something. And I always got it confused and then I started doing this consistently and what's funny is just two days ago I typed the wrong one, and I'm like ‘why isn’t my code working, everything is broken’. It's just you know, like that’s why why document exists, it’s not necessarily just so you can learn but it's also so you can refer to things. And really, I think the most important thing I guess what are you doing frontend web development or computer or anything or data science or anything, is thinking critically about how to put things together rather than memorizing exactly the way to do everything, cause you can always look up the ladder but the former that's kind of where the heart of it takes over and where you need it a human to do it. |
| Ben: | Yeah, I think that's totally right, especially I think data science, this can be particularly challenging because data science isn't a pure field, it's like a little bit of computer science and a little bit of statistics and then there's machine learning, which kind of got elements of those both as well and it borrowed ideas from that originally came out of... feels like economics and physics, and all kinds of other stuff and so it's just as big mishmash and you can end up having ideas that are fundamentally really similar, but that have different names of just depending on somebody’s background they might call it something completely different. And so I think that the vocabulary challenges in data science can actually be a lot more significant than in some of these older pure fields cause it just doesn't have... it hasn't been around long enough for there to be a dominant vocabulary that's really taken hold.
