| Person | Transcripts |
| :-- | :-- |
| Ben: | Hey Katie |
| Katie: | Hi Ben |
| Ben: | I’m gonna read some lyrics. I want to see if you know them. |
| Katie: | Okay. |
| Ben: | It's astounding, time is fleeting. Madness takes its toll. |
| Katie: | Ha ha ha ha… |
| Ben: | So you know what I’m saying? |
| Katie: | Ah yeah, are we going to do the time warp again? |
| Ben: | Let's do it. You are listening to Linear Digressions. |
| Katie: | I love that song. |
| Ben: | Yeah, and then again, part 2. |
| Katie: | Okay, so what are we talking about? Today we’re talking about dynamic time warps. |
| Ben: | Of course, hence the song. |
| Katie: | Yes , and I think it may be my favorite algorithm, one of many that I had it so far. Let's do the time-warp again so this is actually kind of a re-record of an episode that… gosh… when did we talk about this? |
| Ben: | We recorded that a really… I can just say a really really long time ago. It was one of the… |
| Katie: | I remember it’s done in the Udacity offices yeah. |
| Ben: | It’s one of the very early episodes that we recorded. And this one just somehow I never got released for some reason. |
| Katie: | This is the long-lost what's the word I'm looking for. |
| Ben: | I don't know. Cousin? |
| Katie: | No, it's not a prequel...sequel! Sequel! |
| Ben: | The long-lost sequel to the um detector. |
| Katie: | Yeah, which in case you hadn’t... forget this. Okay, so we should take a step back for just a second. If you were… |
| Ben: | Yeah, what was that about? The um detector. |
| Katie: | If you were one of our more recent subscribers, so in early days… |
| Ben: | Which you can go back and you can listen to it if you want to. |
| Katie: | Oh yeah, for sure. I had this vocal affectation, we both do, where… when I need to fill a gap while I'm thinking I tended to say um and a lot of time these ums will be sort of edited out because they don't add anything and a lot of time… |
| Ben: | It’s part of the podcast editing process. |
| Katie: | Yeah yeah. But I noticed when I was editing that when you were editing the vocal files they have that... sort of a picture that rolls in front of you and it’s the picture of a sound wave. |
| Ben: | Yeah, it’s called the waveform, and it’s a visual representation of the amplitude of the sound so if I were to clap right now that would be represented as a sharp spike, and if I say um then that's just kind of a little spike with some silence at the beginning and silence at the end. |
| Katie: | Yeah, so I was noticing that when I say um, yeah, the sound right now, I'm watching myself record. When I say um it has a very distinctive shape to it. It looks kinda like… it always looks to me a little bit like a tornado that’s flipped on its side, it has a short edge in it that it kind of tapers down. |
| Ben: | Oh yeah. |
| Katie: | And the sharp edge can go up very quickly or sometimes it has a little bit more of a slope to it. It can have different lengths obviously, I can say um, that is very short, or I can say ummmm, and that’s a much longer… |
| Ben: | And this matters because when you were recording, sorry, when you were editing, you were looking for the um’s to remove them and you notice that you could actually see them and almost predict where they were. |
| Katie: | Yeah and so my thought was... let's just make a machine learning algorithm that could remove all the ums and wouldn’t that be fun? |
| Ben: | And I think your excuse was because I say because we say um a lot, but I don't think you really need an excuse to take on a project like that, like we're both geeks. |
| Katie: | Yeah, well totally. Yeah, I mean, let's be honest, editing out the um’s is way easier than actually building an algorithm that will automatically edit that out for you. |
| Ben: | The benefit of saying um a lot is that that means that you have enough um’s to make a training set to train some algorithm to … you know … to take out the um’s in the podcast. |
| Katie: | Yeah, and so my heuristic in thinking with this would be a good problem well, good interesting problem for machine learning is that if there's a pattern that I can detect with my eye basically, then a computer should be able to figure that out if I find the right representation for the data and the right algorithm and so on. I'll be honest, it's really not about the algorithm on this one. I think it's much more about figuring out how to represent the data in a way that goes in and that is amenable to the types of algorithms that we have in machine learning. And this is sort of a time series problem. It also very quickly gets into the realm of signal processing and the dynamic Time Warp is actually a signal processing algorithm that I came across in the course of trying to solve one of the problems with the Um Detector and I thought the Dynamic Time Warp was a super cool name and it's a super cool algorithm and that is where we are today. |
| Ben: | I actually want to take a quick step back and talk briefly about how this data is represented, like how is the sound of my voice represented. And here we’re talking about uncompressed signals. We're not talking about MP3's or compressed audio files cause that's represented a little bit differently but fundamentally with an audio file you have a sample rate. And that sample rate is the number of audio frames or the number of times per second that it records some voltage or some number that corresponds to the position of the diaphragm of the microphone. So as I'm talking into the microphone, this little thing called the diaphragm. This little thin flimsy thing is moving back and forth and creating a voltage which is being recorded by the microphone and sent into the computer and then recorded 44100 times per second with the way that we're recording. And you can tell the computer to record more quickly and you can get a higher quality audio file out of that, like if you're recording a song for example. So fundamentally what this means is for every second of audio you have 44100 different data points. |
| Katie: | Yes, but that is not a format that goes into sorta out-of-the-box supervised learning algorithms very nicely. What you want for a supervised learning type set-up is something that you can stick into a matrix. There are some, you know, maybe details around editing this but basically if you can put it into a matrix you can stick it into a supervised classification problem. However matrices have kind of a standard shape to them, one of the things about a matrix is that one row in a matrix can't have 100 columns while a different row and the same matrix has 1000 columns and yet, there isn't a well-defined number of samples that I want to have that I can call an um. Because some um’s might last one second, and some might last a half-second, and some of them might last 3 seconds and so I have all these different length of audio sampling and they all have sorta the same shape, but the shape can be a little bit different from one to the next and they can also be stretched and squeezed and lining them up on top of each other is actually a pretty challenging problem. |
| Ben: | Right, especially because you don't just have a bunch of chunks of audio necessarily and you're just checking to see if they're um’s. You've got this huge gargantuan audio file and you need to figure out both the right way to slice it up to compare things and also how much to stretch it or squeeze it. |
| Katie: | Right. And so that sort of parsing, figuring out, where the windows need to be to collect samples like examples of what an um looks like, but that's something we will skip for the time being. But the issue of the stretching and the squeezing and getting everything to a standard length, that's where the dynamic Time Warp comes in. And so the problem of the dynamic time warp is basically you have two different time series objects. So it would be... let’s say this thing that I'm about to say, um, and then the second thing that I’m about to say, ummmmm, and I need to... I have these two different things it's ostensibly the same word that I'm saying. They have very different temporal length and they might have slightly different tone in them, so that say... the falling edge of the first um might be steeper than the falling edge of the second one, or there might be some undertones in one of them things that maybe you couldn't even hear. But that would make it so... that if you were to just naively stick one on top of the other... it would say, “Oh, it doesn't look like these time sequences map up that well.” So therefore we don't think that these are really the same thing when in fact they’re the same thing.” And so you have to figure out how to probably stretch the short one or squeeze the long one and also maybe move around some of the substructure a little bit… stretching it and squeezing even within the sound of of the um… in order to get things to line up properly. |
| Ben: | Oh I see, so that's a really really tricky problem. |
| Katie: | Yeah, so that's what the dynamic Time Warp does. It’s basically an algorithm the way that when you can think about it visually a little bit is... imagine that you have... let’s say for the sake of argument that the first um that I said there was one second long, and so it was 44100 time slices. And then let's say that the second one is 2 seconds long and so it's 88200. So what you do is you make... you make a rectangle and it's 44100 along one axis and is 88200 along the other axis, you sort of like have one of your time series as the rows and then the other one is the columns. And then what you need to do is you need to come up with something that's sort of a compromise and between the two that gives you a mapping from a point in um a into its sort of analogous point in um b. So you kinda have to find a path sort of along the diagonal of this rectangle that you just made. So if there's no particularly weird substructure if they have literally the same number of, or I should say, the same pattern of samples in them but one of them just takes twice as long. Then what is going to look like is for every step that you make in the short um, you take two steps in the long um. And then you just kind of like move along one step of A, two steps of B, one-step of A, two steps of B and sorta things line up kind of nicely along with the diagonals of that traces out in a rectangle that you’ve made. |
| Ben: | Right, that makes sense. |
| Katie: | Yeah, and yeah so...but then you can imagine that it's not always the case that there's. you know, this easy mapping that you can find that it in, there might be some of that internal stretching and squeezing that we were talking about. And so then what ends up happening is that you wander off of the diagonal. So you might have to do something like you take one step of A but you might have to take three or four steps of B, just sort of end up in the same place. And so then you would wander a couple more squares off of the diagonal as you're not moving forward in A but you are moving forward several steps in B. And so you can imagine there's a number of different versions of sort of the path that you can trace along that are similar to the diagonals but not exactly along the diagonals. |
| Ben: | There may be a little more squiggly or something like that. |
| Katie: | Exactly, and so the dynamic Time Warp is basically a way of looking at all the different squiggly paths that you can trace out and finding one that sort of maximizes the joint likelihood across the two different time series at the same time. So it's kind of solving the problem of what's the best compromise that we could find of all the compromises that are available to us, so that we may end up starting at the same place and ending at the same place and then everybody's been stretched and squeezed so that we can come up with some sort of agreement about what happens in the middle. |
| Ben: | So Katie, what if we… what would it sound like if we ran your own detector on this episode? |
| Katie: | Oh, ah... (laughs) |
| Ben: | So apparently we haven’t? (laughs) |
| Katie: | Oh, no, no. Yeah, guess who's got two thumbs and isn't really working on this anymore? This gal. |
| Ben: | Oh I see…(laughs) |
| Katie: | Yeah, no… it’s interesting...ummmm... yeah, it was interesting to get all the audio reading in and to get some of that dynamic time warp software up and running and then I kind of don't care that much. |
| Ben: | Oh… |
| Katie: | It’s so much easier to just not say um that much... |
| Ben: | Yeah. |
| Katie: | Than it is to come up with this algorithm. I mean, it‘s fun and it’s interesting and things like that but there are other things that I’m more excited about. Software up and running and then I kind of don't care that much it's so much easier to just not say that much and it's with this with this I mean it's fun and it's interesting and things like that but there are other things that I am more excited about. So it's pretty back burner at this point, I haven't worked on it a long time. But I also... I always hated it when I was a kid and I would watch TV shows and there would be some kind of interesting thing that got started at the end of a TV season like a... |
| Ben: | Like a cliffhanger? |
| Katie: | Yeah, and then it just starts up the next season and it never acknowledges that there might be this thing… |
| Ben: | So that’s what this episode is? |
| Katie: | Yeah, it’s me... kind of working through my cathartic moment of acknowledging that I'm never going to finish the side project but it's really interesting, so it's worth talking about. |
| Ben: | And we got two episodes outta it, so that's a win. |
| Katie: | Yeah, that’s right.
