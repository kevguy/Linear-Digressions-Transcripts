| Person | Transcripts |
| :-- | :-- |
| Ben: |Hey Katie. |
| Katie: |Hi Ben. |
| Ben: |Today, do you want to talk about terms? |
| Katie: |Terms? |
| Ben: |Terms, and when they can be a little bit fuzzy. |
| Katie: |I feel like you're really kind of mining for some little nugget of an introduction here. |
| Ben: |Yeah, I just... |
| Katie: |And I'm not about particularly impressed with (inaudible) |
| Ben: |Hey, let’s talk about data mining, how about that? |
| Katie: |That sounds good. |
| Ben: |Alright, cool. You are listening to Linear Digressions. |
| Katie: |Oh, I saved that intro, didn’t I? (laughs) |
| Ben: |You really did, yeah. I was struggling there. Actually before we dive into this, this is... this is a topic that a listener suggested that actually just got me thinking... one thing I'd like to do is, if you were listening to this podcast and you left us a review on iTunes... high five... you are awesome. If you haven't, whether you like us or you hate us, go leave a review on iTunes. It made me so happy, I was just looking at the reviews a couple days ago. I think we have 32 reviews or something. |
| Katie: |Nice, cool. |
| Ben: |Yeah, 32 people love us or hate us, that’s awesome. |
| Katie: |Woo. |
| Ben: |So be one of those people. Listener’s suggestion, you wanna tell us about that? |
| Katie: |Yes, so this is coming in from a listener named Rachel. And she was asking about data mining, so sounds like she’s a little bit maybe newer to this field than some people who have been around for a long time… |
| Ben: |But why do you say that? I mean, I don't see any problem with the term “data mining”. |
| Katie: |Oh, so data mining… I think the reason I say that is because “data mining” A. is a little bit of an older term for some of the stuff that happens in data science, and the things that she was asking about, pointing it out rightly… I think... is that data mining has a little bit of majority sense sometimes as its use today. It’s seen as like not really good data science to be doing Data Mining, and so she's asking... I think... a really good question which is like, first of all, can we just talk about this a little bit what it is, which is really hard to do, because it's one of those fuzzy terms and everybody means something a little bit different. But... but then also to unpack why sometimes people say it like it's a bad thing. |
| Ben: |Yeah, so what exactly is it? I mean from the term it kind of sounds like... kind of an exploratory thing, you've got a bunch of data, we've got a bunch of data, and you don't... I guess you don't really know what you're looking for, and so you kind of rummage around a little bit and... and dig through it and look for patterns, and that's what comes to my mind and what the term kind of implies to me. |
| Katie: |Yeah, that's what I think of too. And especially because of mining the idea that you have this big pile of dirt and then there might be a few really interesting little nuggets that you can find if you hit it in just the right way. |
| Ben: |Right. |
| Katie: |And one of the big problems that can happen with this is, if you try enough things on a big dataset, you're going to find interesting relationships and it's not because there's necessarily relationships there, it's just because you can get fluctuations in the data that happen to look like, you know, they might be predictive of something interesting, but they’re... but they're really not. |
| Ben: |Right, we talked so much about different methods for ensuring that what you think you're looking at is actually what you're looking at, making sure that the signal was not necessarily noise and ways of checking data, and if you were just rummaging around in a dataset and looking for patterns, by definition you're not doing that, because you're not looking for anything in particular, just... you're just looking for patterns, and you're looking to infer things from those patterns, which may not be there. |
| Katie: |Yeah, you can end up fooling yourself really easily. And so, I think there was a lot of... well, still is a lot of bad data science that happens when you just kind of let yourself loose on a dataset, looking for interesting patterns as opposed to sort of having a more structured hypothesis or an idea of what it is you're looking for, and what it is you're expecting to find, that sort of a better way of not accidentally fooling yourself into thinking that you found something when... you know, you’ve gone fishing and you think you've got a big fish and you just pulled up a boot. |
| Ben: |Right, yeah. |
| Katie: |Which I think it's the metaphor that we used the last time when we talked about this. So apparently I’m a one-trick pony when it comes to metaphors about data mining. But another thing that I was reading about, when I was reading up on data mining this weekend that... I think, it’s even maybe a little bit more interesting is the topic of how within data mining, generally within certain kinds of data analysis, you can sometimes have this problem, they call it sometimes data leakage, which I think sounds really gross so we'll call it “data leakage”. |
| Ben: |Yes. |
| Katie: |Which didn’t sound that great either, but it’s a little cringe-inducing. |
| Ben: |No, no, it’s better than leakage. |
| Katie: |Yeah, I agree. But here’s the rough idea, is that a lot of times you’re in a situation you wanna do a supervised learning problem. So you have one feature in our data, a column, and it's sort of this target or like... this truth quantity that you want to be able to predict, given all these other features that you have in the data. And you know… you know in a perfect world, there's some relationship between all of your predicting features and your outcome variable. Otherwise, you know, what are you doing? You're not gonna be able to find anything. So there has to be some kind of relationship, but sometimes there can be... like I said, kind of contamination where your labels can actually sneak into the features themselves. So an extreme example of this is, imagine you have a machine learning algorithm that just has the the target outcome as one of the features that you have available to you when you learn, well then your algorithm is definitely gonna pick up on that, and it's just gonna say, oh hey, there's this one column, this is perfect, so we're just gonna do that. |
| Ben: |You’re just gonna use the column that has the answers. |
| Katie: |Yeah. |
| Ben: |It’s kinda like if you have an answer cheat sheet along with a test. I mean, are the students gonna look at the test, no, they're just gonna look at the cheat sheet. |
| Katie: |Exactly, and this can be a really easy mistake to make. I made it, and you have to have a healthy skepticism of models that look like they're doing a little bit too well in order to find it when this happens. So there's no shame in doing it, the only shame is in not being properly skeptical of your models when they they come back and they say something like I have 100% accuracy. |
| Ben: |Oh yikes. And I mean, you know, that could be applied to life as well. A healthy skepticism is always good. |
| Katie: |I think skepticism is one of the most valuable skills that one can have as any kind of scientists. |
| Ben: |I mean, actually just go on a brief digression here. Skepticism is effectively critical thought, right? Critical thinking paired with... an inherent mis… not mistrust, but not taking everything necessarily its face value, thinking about those things critically. And actually when people are talking about things like a learning programming, that’s near and dear to both of our hearts, because we used to work at Udacity and Udacity teaches programming and other such things. |
| Katie: |Mm hmm. |
| Ben: |You need that critical thinking, and often times, people kind of they’ll learn programming language but really what they should be learning is the critical thinking around it, like the ways to attack a problem, an algorithmic problem or something like that, that's really what you want to focus on and learn, not something like the syntax of a particular language or something. Is that too digressioning? Pretty digressioning? |
| Katie: |Well, that's okay. No, I think it's... it's a good point, and it's something that we very often get… very you know, we do this on the show, everybody does this, we get pretty enthusiastic about stuff. And in general, we try to find things to talk about, that we think are, you know, of high quality and that are kind of have been tried and tested and have been found to be pretty robust, but yeah, in general, you should be... you should be thinking critically about all the things that you read, this came up just in the last episode or two episodes ago where we were talking about that one guy that did the study of talking with people about transgender issues and when you were back in a little bit you were like, well I don't like, it’s a good study, he didn’t do anything wrong, but I don't know if we should get overly excited about this cuz we might be... just you know, detecting something like that, he was managing to find people who are more open-minded rather than really managing to change people's minds, and so just thinking about it, particularly trying to make your own evaluation of what's actually going on here. But anyway. So I was reading a lot about this contamination that you can get sometimes from... from your variable, interest into like the training feature that you have available to you and this has happened a lot more than I had realized, sometimes at very high-profile like competitions and some things like that. As you may know there's a number of different kinds of data science or data mining competitions that are hosted on sites like Kaggle and DrivenData, and then also there are companies sometimes that will sponsor big competitions every year. And there’ve been a number of these where the data hasn't really been properly sanitized so to speak, and you actually get kind of bleed through from the the target variable into the dataset. So one very interesting case of this was in 2008. There was a competition called the KDD Cup. And what they were trying to do was they had a bunch of features that are relating to breast cancer patients, and then that the outcome you were supposed to be predicting was whether this person had some kind of breast cancer or not. |
| Ben: |Yeah Right. |
| Katie: |So you can imagine you have a big table and it has sort of this index column that has a patient number in it and then a bunch of attributes about the patients and then whether they have breast cancer or not. And as it turned out there was actually... there was a relationship between the data collecting process and whether people had breast cancer and collecting process was reflected in the ID numbers as they were assigned. |
| Ben: |Woah, so the algorithm didn't necessarily have an answer key of does this person have breast cancer vs. this person not a breast cancer but encoded in the patient ID that information existed? |
| Katie: |Yeah. |
| Ben: |So maybe... the... I mean, I don't know, how they did that but maybe they said like patients with breast cancer and get this particular ID range or something? |
| Katie: |Yeah, so what happened was it was a an agglomeration of data from several different sources, and some of the sources had much higher likelihood of having people who actually had breast cancer being referred to them because they're... you know, maybe like in a different place that... this is the hospital that you get sent to if you already are exhibiting strong signs of breast cancer whereas other doctors’ offices are just getting some... you know, a group of people that are not as in which people who have cancer. And so then, the people who go to those hospitals then get assigned certain IDs and then all of the IDs that are associated with that hospital, they have a certain pattern in them... or there were… you know, sort of concurrent one after the other in the table and then the algorithm was able to figure out based on the ID whether someone has cancer or not, which of course is like, not going to be anything was going to help you if you're trying to figure out for a new patient if they weren’t generalized very well. But people were really surprised. |
| Ben: |Right, what’s your ID number? Wow, that’s really tricky. There're so many potential blind spots like that. |
| Katie: |Yeah. |
| Ben: |And they might be obvious once you hear about them or once you discover them, but before you discover them, the blind spots… you don’t know they exist, and so, I guess that's where that… the healthy dose of skepticism comes in. |
| Katie: |Yeah, so for a case like that, the thing that you should always do if you have a model that seems to be performing very well, you should always do this in general, is a lot of packages will support some way of understanding which are the most important features that your algorithm has identified. And so in a case like that, if you see that the patient ID is coming back as one of your most important feature, that’s a huge (inaudible) that something weird is going on. |
| Ben: |Right. |
| Katie: |And then there's another kind of contamination that can happen a lot and I think this is also worth thinking about, which is a lot of times there’s data that’s time series data, which we haven’t talked about time series problem so much, but it's basically data that’s coming in… you can imagine sort of a streaming format of data where every time interval you get a new data point. So an example of this would be like audio data, stock price data... that’s all I can think of right now for some reason. But the idea is based on what I've seen so far, predict something about what's gonna happen in the future, and so there was an example of this, this was in 2010, there was a competition called INFORMS and it was kind of a standard stock price prediction competition. And the thing that can be really nasty about this is that with time series data you have to be really careful to not look into the future, which sounds... like duh, but really hard to tell there can be a sort of complicated little ways that you can accidentally build yourself a little time machine, where you can actually see into the future in ways that you don't anticipate. |
| Ben: |So you're talking about... you're talking about taking... trying to take a dataset and only look at let's say the first half of your data over this period of time and then trying to make a algorithm that is predictive, that can kind of predict the future, but you need to have that algorithm only look at the data from the past, not the data it’s trying to predict, either directly or indirectly. |
| Katie: |Yeah, that's right. So let's say that you have stock prices for Monday Tuesday Wednesday Thursday and then the problem is... given this information, predict for me what the stock price is going to be at the end of the day on Friday. |
| Ben: |Oh, you know what? Can I make a guess? |
| Katie: |Mm...hmm. |
| Ben: |Because you phrased it this way, every Monday has information potentially about the proof... the previous Friday. |
| Katie: |Ah... |
| Ben: |Is that… |
| Katie: |So the thing that I was actually thinking about is sometimes you have... it’s even more basic than that. It's like sometimes you have weak level, W-E-E-K, week level attributes that you calculate. |
| Ben: |Okay. |
| Katie: |And so then if you have those attributes that contain information for the entire week, but you're nominally only supposed to know about the first 4 days of the week, then that can be a source of contamination at your target feature. |
| Ben: |Oh I see. |
| Katie: |So in particular in this stock price prediction thing they had this...you know... contamination problem. There were a couple things that were going on. Number one is that the stocks were de-identified but they did have some information about… say the sector that they belonged to, and then so what you could get is... you could get future type information about other stocks that were in the same sector as your target stock. So let’s say that you have a stock and you don't know what oil company it is, but you know it’s an oil company, and you're trying to predict what it is going to do on Friday, based on Monday through Thursday. But you also happen to have a bunch of other oil stocks that you have the full week's information for, and you know that there was also some economic report that came out the first thing on Friday morning, and so based on what else is going in… going on in that group of oil stocks, you can make a prediction that is a little bit peeking into the future that... the... that the price of the stock that you're interested in is going to go down because of all these... these other stocks the sort of all move together that are telling us it’s going down. |
| Ben: |Right, so there’s sort of a hidden correlation, and i guess in this case the correlation seems fairly obvious but still it could be easy to miss. And I'm sure in other cases there could be correlations that are much much more difficult to imagine. |
| Katie: |Right, and then another thing that they raise here is that sometimes with these competitions it's a little bit of... there was a little bit of a fuzzy line between whether just only the competition data was allowed or whether there was also the option for competitors to find outside data sources and join them in as well. And you can kind of understand both sides of that. On the one hand, only having everyone using the same dataset kind of keeps the playing field level in a way and it also avoids any problems that are like... let's say in this case like a stock being re-identified, you can figure out actually… un-label stock... which stock it was, and then you can just go look it up and see what the price was on the other day... on Friday. |
| Ben: |Mm hmm. |
| Katie: |But a lot of times the companies that are sponsoring these companies… the reasons that they’re sponsoring them is they wanna see what a whole group of people deal with this problem when they are sort of doing it the first time. They’re looking for new ideas basically for sources and insight. And sometimes, finding new datasets is gonna give you some new insights, insights that are legitimate. And so… |
| Ben: |Right. |
| Katie: |One of the things that this paper proposes as a way of avoiding... you know, the worst of these problems, especially in these competitions is these competitions you just have 2 tracks, one of which is you're only allowed to use the data that's provided for you as kind of like the starter data, and then maybe a second track where you're allowed to join in other data from outside sources and then that way it's very clear... if you see much better performance with the outside data that might be what's actually responsible for and people can just be a little bit more thoughtful about what that new data might be bringing to the table. |
| Ben: |Right, yeah. |
| Katie: |And then the other thing that I thought that they had that was fairly interesting as a way to avoid this is a big problem is... don't try to take a dataset where you know the answers and then artificially try to remove those answers cuz... even if you think you've done a good job. it can very often be that you haven't totally removed them or you either move them where you thought they were, but they've kind of migrated to somewhere else, where they're even harder to find. |
| Ben: |Mm hmm. |
| Katie: |And so what I mean by this is instead of taking... let's say the stock prices from last week, and say based on Monday through Thursday tell me about Friday and I'm just going to ask you to kind of close your eyes and pretend you don't know anything about Friday, run a competition where we're actually... today... trying to predict the stock prices for tomorrow and truly no one actually knows. |
| Ben: |Yeah. |
| Katie: |And that's a much more honest metric which I think was a really good point is that at the end of the day, it's these algorithms are interesting because they give you some insights into what actually can happen in the future, so just set it up so that you're dealing with cases that actually happen in the future. So... so that was how I spent my Saturday, I was reading about data, we've managed to avoid a whole episode without saying the term “leakage” so… |
| Ben: |Oh yeah. That’s... |
| Katie: |(inaudible) I’ll say. |
| Ben: |Well done well done. |
| Katie: |Yeah, but it’s a really interesting topic and there's all kinds of additional like... good little case studies that you can find out there on this. And then one other thing I'll say is Claudia Perlich is one of the data scientists who I think talks about this a lot and she has some really... the paper that I was reading was one that she and a couple… a couple of collaborators wrote, it's called “Leakage in Data Mining: Formulation, Detection, and Avoidance” and so in general, this is something you’ll find interesting. She has all kinds of interesting things to say about it.
