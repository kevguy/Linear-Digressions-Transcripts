| Person | Transcripts |
| :-- | :-- |
| Ben: | Hey Katie. |
| Katie: | Hi Ben. |
| Ben: | I was hoping we can play a little game. I’m gonna try and guess what we’re talking about today, and you tell me ‘warmer’ or ‘colder’. |
| Katie: | Okay. Yeah, I know this game. |
| Ben: | Yeah, alright. I think everyone’s played this game. It’s a bit pointless (inaudible) So are we gonna talk about an interview done with someone that pertains to data science? |
| Katie: | Colder. |
| Ben: | Okay, it’s (inaudible)… it’s the wrong... |
| Katie: | Yeah, try it again. |
| Ben: | Are we going to talk about some kind of algorithm? |
| Katie: | Warmer. |
| Ben: | Okay, are we going to talk about some sort of learning? |
| Katie: | Much warmer. |
| Ben: | Ooh, that's good. Okay, are we going to talk about teaching data science? |
| Katie: | Colder. Well, the way that I think you mean it, colder. |
| Ben: | Yeah, yeah, okay, alright. Go back to learning. Are we going to talk about reciprocal learning? (Laughs) You're very warm in terms of... the... what the word sounds like, not that reciprocal learning is a thing but… |
| Katie: | Alright, ok, alphabetically I'm close. |
| Ben: | Yeah |
| Katie: | Yeah... actually I already know the answer right? This is all a gag, but um... Reinforcement Learning. |
| Ben: | Reinforcement Learning. Yes, very hot. You’re on fire! |
| Katie: | Alright, you’re listening to Linear Digressions. |
| Ben: | Okay, would you care to tell me why I played that game? |
| Katie: | (Laughs) Yeah, so the idea was to have a little bit of the illustration of what Reinforcement Learning actually is about, cause I’ll be talking about Reinforcement Learning, this is kind of the first in... in a two-parter of sorts... so this one is where we'll summarize a little bit of what's... what are some of the problems that people are trying to solve with Reinforcement Learning, what is Reinforcement Learning. |
| Ben: | Okay. |
| Katie: | And the idea of Reinforcement Learning is it is a kind of Semi-supervised Learning, so you don't have labeled training examples, the way that you do with a fully supervised classifier, let’s say… or a regression model. On the other hand, you are getting some kind of feedback from your environment, so you can’t in a way figure out if you're kind of moving in the right direction and... but that... that... supervision is not particularly... it's not really stronger than it is with supervised... supervised cases, so, and we'll talk about sort of what I mean by this. And so what we did there in the introduction, the kind of “hotter, hotter, colder, colder”, is you’re exploring the space a little bit of one of the things that we could be talking about, and I'm giving you guidance, kind of bumping you back onto the path that is going to eventually lead you to the correct answer. And giving you some encouragement… yeah. |
| Ben: | Yeah, so you’re kind of like giving me positive/negative feedback? |
| Katie: | Exactly, and in that case, I was giving you a... sort of an unfair advantage that many reinforcement learning systems don't give to their... to their little... AI, artificial intelligent, you know, little agents in it. Every time that you asked a question I answered it completely honestly and correctly, there was no uncertainty that you were getting the wrong information from me when of course, sometimes when you're interacting with a complicated environment, you can get sort of noisy results back. So Reinforcement Learning is also designed to deal with uncertainty in the algorithms themselves. |
| Ben: | So maybe could we start with with an example of... I don’t know, like I mean, one that I feel like I might connect with is the Roomba that’s trying to clean your house or something. |
| Katie: | Yes, it’s Reinforcement Learning. Yeah, a lot of the examples and the early use cases are in robotics. And we’ll talk a lot more about the Roomba In the next episode. But… |
| Ben: | Oh really? |
| Katie: | Yeah, yeah. So, but the idea is you have a Roomba and let’s imagine that the Roomba is not something that is explicitly programmed to know its way around your house. And in fact, let’s generalize it a little bit more, because a Roomba just sort of has one task, which is, cover the carpet with your little 4:20 |
| Ben: | Imagine that the robot has a more general task which is clean my apartment. |
| Katie: | Yeah. |
| Ben: | And… |
| Katie: | And like… are we imagining like robot like Bicentennial Man Robot, that's, that's like bipedal and is walking around your house and kind of naively interacting with the world that has maybe fewer of the constraints that everybody knows that a roomba has, like the roomba kind of obviously has a limited ability to sense its environment whereas if you imagine like a pseudo humanoid robot that's just very naive... maybe that problem becomes all the more generalized. |
| Ben: | Well the limitation is really your imagination here. |
| Katie: | Okay. |
| Ben: | I think that the Roomba is more realistic in any kind of short-term situation but reinforcement learning is used in all different kinds of Robotics in fact there are robotics systems that do things like learn how to walk, and how to recover their balance if you try to push him over and things like that and very often those kinds of complicated dynamics of how it actually goes through the motions. A lot of times it will learn, at least in part, using reinforcement learning systems. But imagine that it's... yeah but let’s imagine it's somewhere in between a Roomba and C390 that… why don’t we say R2D2? I think that’s a good mental model. So it’s … it can. |
| Ben: | Alright yeah. |
| Katie: | It can roll around, it's got a little bit of like appendages that I can stick out, but it's not likely humanoid. Okay, so R2D2 was in your house and it wants to clean your house for you and it's it's brought into your house not knowing anything about what's going on in your house, it just knows I'm supposed to clean things. And so the first thing that we should point out is that this is an autonomous agent. So it has to figure out what is supposed to do in order to clean things and the way that it decides what to do between the different options that it has is, it has something called a reward function and so the reward function is going to be some kind of mathematical formulas programmed into the algorithm itself: this is what you want to do if you want to optimize for Ben's house being clean and we have to think very carefully about how to define that exactly and that's what we'll talk more about in the next episode, but for now I just want to introduce the idea of the reward function being something like clean house. |
Ben: | Katie, so my reward function in the example in the introduction would be if you say warmer or something like ‘warm’ or ‘hot’ or ‘really hot’, that's good. And if you say ‘colder’ that's bad? |
| Katie: | Yes. |
| Ben: | So really really simple? |
| Katie: | Yes, and so now the R2D2 is going to wander around your apartment trying things, and sometimes those things are going to work, sometimes are not going to particularly work, but it's going to be collecting feedback from the environment, so it might be something like bumping into a wall, and that's a piece of feedback and it knows that now there's a wall there. Or it might learn that there's a set of stairs and it should stay away from stairs or it might learn that... |
| Ben: | I'm really sorry, I just have to say I'm really wishing that I had an R2D2 soundboard in front of me. |
| Katie: | I’m really liking this example. |
| Ben: | (imitating sound of R2D2) |
| Katie: | And so what R2D2 is going to try to do... there are several different flavors of Reinforcement Learning, so we’re really just kind of skimming the surface in one of them, but you get sort of some of the high points here. What it’s generally going to try to do is learn what’s called the policy and a policy is a set of rules that it figures out. That says in a given situation what am I going to do. So for the case of something like a cleaning robot, if I come up on some kind of mess, what am I going to do about it? Or how am I going to prevent future messes let's say... from happening? So you could have something with a little bit more complicated about... I see that the window is open and it's very windy and there's a stack of papers right in front of it, and maybe I should close the window instead of clean up the papers, you know, there's... there's a bunch of different ways that I could interpret things in, and come up with what’s called the policy as a result of all of those heuristics that it learns. And then those interact with a reward function because what it's trying to do is come up with a set of actions that I can take that will maximize the reward function based on sort of the probability that calculates of success of various actions. So one example of this is, let's say, you have your robot and it wants to clean up your house and see if there's a pile of Cheerios that have fallen on the floor, so little cereal pieces. And let's imagine that you're a robot that really doesn't know anything about the world, how are you going to... what are some of the things that you could imagine trying... you've been programmed with maybe some stuff about some heuristics for how to wash dishes and how to clean a toilet and how to sweep a floor and how to dust the furniture, but you don't necessarily know what any of those things mean, you just know the actions and so what are... |
| Ben: | Alright. |
| Katie: | What’s it gonna look like when you try each of those things on the Cheerios on the floor? |
| Ben: | I see, this could end... this could end with failure in a lot of these cases like, I imagine let’s say our AI R2D2 decides, “Hey there's a mess in front of me, I know that in some situations, cleaning things with a wet sponge would help, so maybe I should try that?” And then you end up with just more of a mess and your Cheerios are everywhere and you've got water all over, but then, I mean that's presumably a negative thing in terms of the reward function. So the robot is de-incentivized from doing that again, and then maybe another time I might... the robot might try sweeping it up with... with a broom and dustpan or something, and... |
| Katie: | Right. |
| Ben: | Maybe that works better and so it's more incentivized to try that in the future and maybe even alterations or something. |
| Katie: | Right. And so… there’re a few things that are really nice about this system then, is you just have to start out by... maybe... link some fundamental ground rules like what are the things that you're allowed to do and a reward function that you should be thoughtful about... but once the reward function is set, then the little robot is going to go about figuring out how to actually accomplish those tasks. And so, one of the things that's nice about Reinforcement Learning is that you don't have to necessarily explicitly pre-program it about how it's going to deal with every specific situation it comes up with. What it is going to do is start exploring all the options that it has and learning about what seems to pay off and what doesn't, and then at some point, you start to transition into a little bit more of a exploration versus exploitation trade-off, so this is a little bit like what we talked about with Multi-armed Bandit problems and things like that, that you always have a choice of ‘do I want to try something new’ or ‘do I want to stick with kind of the best available option as I'm aware of it right now’. |
| Ben: | And when you try something new, that means you're gathering more information but it also means maybe doing less ideal of things. |
| Katie: | Yes. |
| Ben: | So the robot could keep trying all kinds of like... maybe try Windex on the cereal, maybe try all kinds of things if it's doing a lot of exploration vs. exploitation, that also means it's going to be a lot less effective during that period of time… |
| Katie: | Yes. |
| Ben: | And so eventually wants the robot has a lot of information about the things that work and the things that don't, that doesn't... it will probably want to shift its function from doing more exploration to doing a little more exploitation. Otherwise, you know, the human owner is going to be like this robot is just doing crazy stuff and it never cleans my floor so… |
| Katie: | It's just... it's just learning about itself, Ben. It's discovering the world. |
| Ben: | Actually, I just need to say one thing about that. |
| Katie: | Mm hmm. |
| Ben: | The whole time we've been talking about this, this all seems very unremarkable to me, and I was thinking like ‘Why is that? Have I heard of this before?’ And fundamentally I think I've been doing this my whole life, it seems that this is exactly the way that humans explore the world and learn about the world. You know, like you come out of the womb, and you don't have a script and you don't even have a very clear reward function, you kind of figure that out based on the way other people around you react to your actions. And also you don't even have a clear set of options, you know, babies are just kind of move their limbs around and see what happens. And then eventually those actions become more constrained as they figure out what yields actual rewards. |
| Katie: | I think that's a really good point, of all the kinds of machine learning that I've learned about, Reinforcement Learning always feels the most human in a way and I think that that's reflected in the fact that the things that we really think of as artificial intelligence, many of them have reinforcement learning running under the hood, and not as strict supervised classification or unsupervised learning. So I just to come up with like a quick list to be concrete about what are some of the things that are cool going on in Reinforcement Learning. First of all is Alpha Go, the game of Go, sort of like chess but harder for computers to play ostensibly. |
| Ben: | We had an episode about this. |
| Katie: | Yeah, we had a couple, we had a sort-of full-blown episode and then I think we had an update episode when it played the really good human a couple months ago. So Alpha Go is this artificial intelligence that was optimized for beating humans at Go which it was able to do a couple of months ago, much faster than anyone predicted the computer was going to be able to do that. That's based on different Deep Reinforcement Learning. There's also aspects of self-driving car programs and there are several of these that are being done at different institutions and so each of them might have their own interpretation of exactly how to do self driving cars for reinforcement learning is a major part of that, you get guidance from the driver or from the system, sort of the environment that the car is in, telling it what a stop light means for example, that the car could figure that out on its own. Third sample, we've already touched on a little bit, is robotics. A lot of the early Reinforcement Learners are people here trying to figure out how a robot that can be dropped into a new situation and start exploring the environment and figuring out what is supposed to do in that environment. And then another one that you don't always hear about quite as often in sort of the machine learning data science sea circles, but is in another place where this has been going on for a long time, is in a field is called Operations Research. Operations Research is centered on a lot of times things like optimizing a supply chain or you can imagine like an assembly line, where there's many points that are all intertwined together, and each of those points can fail potentially, and the failures from one point can propagate through some of the other points, and so then you don't really have an agent like a robot but you do have a policy that you can come up with in terms of what kinds of let’s say, in the case of an assembly line, you can be keeping reserves of inputs for one machine, so that if the machine that's one step... upstream of it goes down, that machine… the downstream can still keep on working even though you know, there may be a little of pause in the supply chain, you have a little of reservoir there you work through. Or figuring out if you have some money you can put into repairs, where the places you should go to repair, you can repair the machines that go down the most often, or are you going to repair the machines that are the most costly to the overall assembly when they go down, this kind of optimizations, how do you deal with this sort of complicated uncertain systems. |
| Ben: | That all sounds of interesting. We’re kind of running out of time for this episode, but do you have anything more to say on this topic? Cause... |
| Katie: | Yes. |
| Ben: | We can make this a two-parter. |
| Katie: | Right yeah, so where we’re going with this is kind of introduce some of the idea behind Reinforcement Learning, some of the heuristics, things like reward functions and policies. And what I wanna talk about in the next episode is a really wonderful paper, extremely accessible, like not technical, it’s just fun to read. It’s come out of a collaboration between Google Brain, and researchers at UC Berkeley and at Stanford, and I think maybe OpenAI as well. And what they’re exploring in this paper is where are the places that Reinforcement Learning can go wrong. Not just Reinforcement Learning specifically, but sort of Artificial Intelligence generally. And what are some of the things that we should be on guard against? Not in the sense that like the robots are gonna take over and kill us all, but there are many more mundane waste that you can get something that’s not very well optimized even though you think you’re setting yourself up for success. So we’re going to talk about this really delightful paper that talks about the ways that how R2D2 can accidentally create havoc when it’s trying to clean your house.
