| Person | Transcripts |
| :-- | :-- |
| Ben: | Hey Katie. |
| Katie: | Hey Ben. |
| Ben: | So last week we talked a bit about election polling as it pertains to the US selection but I remember you mentioning that you had some interesting things to say as it pertains to something else. |
| Katie: | Oh yes, so I was thinking about the Brexit that it was taken I guess maybe a month ago or so now at this point and that was, you know, a kind of interesting point of contrast it was again another big national election and there's for the reputation that the polls failed in that one, so I think it's interesting and is worth unpacking. |
| Ben: | Just over the pond, you are listening to Linear Digressions. |
| Katie: | Yeah, so just quickly recapping in case you live under a rock and somehow missed it. So Brexit was a referendum that was running in the UK about a month ago and the question was whether the UK was going to remain part of the European Union or decide to leave. And there was one of those big National referendum that was taken and until even the day of the vote, most of the predictions were saying that they thought it was going to be close but that they thought that the remaining fraction was going to win out, and the UK would decide to stay in the EU. And that is not what happened, but ‘leave’ actually won. |
| Ben: | Yes, that’s not what happened. |
| Katie: | Yeah, and so people were like ‘what happened?’. |
| Ben: | And also how much did ‘leave’ win by? Was it really significant or was it like really really close? |
| Katie: | I mean, I guess it really depends on what you mean by significant or really really close, I think it was something like 48 to 52, so I would say that's fairly close, but not so close that it was like you know... 2000 election, you know, homerun Florida sort of close. |
| Ben: | Right. |
| Katie: | You know we are still talking about like hundred of votes I suppose. |
| Ben: | Also I think that it’s funny to talk about it as was it significant? Well, yes, it was significant, because… one side won and one side lost, you know. |
| Katie: | Well yeah, and there’s also the issue like statistical significance which can be like easy to confuse when somebody says ‘significant’ it depends on if they’re talking about a statistical result or they’re just kind of using it heuristically as like this is important or whatever. |
| Ben: | So I should watch my language. |
| Katie: | Yeah, it's kind of interesting, there’re a few things that I think are interesting to unpack about the Brexit poll and the first one has to do with statistical significance. |
| Ben: | Okay. |
| Katie: | So they have these projections of... we were talking a little bit about polls in the last episode and how it might say something like… well take the case of Brexit, they said 51% expectation that we have for what’s the vote share that ‘remain’ is going to get. So we think ‘remain’ is going to win, but the margin of error on that is let’s say 3 percentage points, so what does that actually mean? And usually what they mean by those kinds of numbers as it'll be something... so there’s 3 percentage points so it’ll will be something like an 90% confidence interval, and nominally what that usually means and it depends a little bit on exactly what kind of statistical methods that are using to drive it but it means something like 90% of the time if we were to run this election hundred times, 90% of the time it would be between... I guess... 54 and 48, so 51 +/- 3. And sometimes the exact details of that will be a little bit different but you get the idea. But what it's also saying is that then 10% of the time we would think that the election could be more extreme than that, like we could be wrong, by more than that and so that 3% just kind of gives you an idea of the uncertainty that's attached to the projection that they're making. So first thing is that there's some really good... the number that I quoted was something like 52 +/- 3% or +/- 4% depending on them... those kinds of polling sources you were looking at. And there's some pretty good analyzes about why instead of thinking about that 3 or 4 percentage points error bar instead of thinking of that as like a 90% confidence interval, it's probably more accurate to think of it as like a 50% confidence interval, and let you know in fact there was much more uncertainty about the projections they were making than what that number would imply, which means that in the case of you have something like 51 +/-6 or 7 then it means it's wide open and like others it's basically a quince look at that point which side is going to win. So we shouldn't be that surprised that ‘leave’ won if there’s a conflict right? There’s a very healthy chance that that could happen. So that's the first thing that we’re talking about, the significance of somebody’s projections to think about that carefully cause it could mean all kinds of things. The second thing about the Brexit polling is that a lot of these models kinda have a couple components to them. They have what we are seeing in the polls right now, like if we were to call a thousand people today or 10000 people today, what are the numbers that we would get versus what if we've been seeing for the past weeks and months? And so you can have these longer-term trends that happened in recent months, but by and large things usually don't move that quickly, and so, having that historical context tends to kind of smooth things out, and when you start to see things that look very different from anything you've seen in the past, very often you know that kind of pass historical context kicks in a little bit and it says it's as effectively like okay we're seeing something different in the polls today, relatives than what we saw last week and a perfectly plausible explanation for that is that we're just getting some sampling error right now, we're getting some statistical noise, and so let's not get excited here, chances are that this is just a fluctuation a sampling thing and it's not actually indicating big changes and public opinion. |
| Ben: | So… I have kind of a larger question which is as a consumer of the end result of all of these statistical analyses, if they have such large error bars then how should I look at these then? I mean, should I... if the error bars are relatively wide, should I look at it as a coin flip or I mean, definitely the way that it's reported on is not always a coin flip, you know? |
| Katie: | Yeah, so it really depends on what your source is exactly, because some sources will do a lot of this work for you. So one of the things that FiveThirtyEight does, the New York Times does, I think actually probably a lot of different poll aggregators that do it at this point, is crunch it all and turn it into a number, something like 75% of the time we think... given the numbers we’re seeing right now, like 75% chance are models giving us that... you know, it's going to go for ‘remain’ in the... when the votes are actually counted. And so then that's a fairly simple thing to interpret obviously, like everybody sort of understands what 75% means, or at least hypothetically they could, it can sometimes be a little bit hard to understand like small probability events, we're not very good at intuitively understanding them but anyway… |
| Ben: | Like 75 plus or minus even 5%, I like, I can... I can turn that into the word ‘probably’? |
| Katie: | Yes, so that’s saying that if we were to run the Brexit election a hundred times, you would expect that 75 of them you would see ‘leave’ winning or ‘remain’ winning, I don't, it doesn't really matter, for the sake of argument. And then of course like if you were to actually run the election a hundred times because you had some kind of like magical thing, then you might see exactly 75 break one way, 25% the other way, but you can start to get sampling, that you know, sampling facts and that, and you might actually see something like 73-27 and/or 78-22 and those would be perfectly... perfectly sensible things to see as well. So you're right, that there is usually kind of an implicit error bar on those meta predictions but usually the implicit error bar is not reported directly, usually the way that it gets incorporated, it's just if there's really big error bars, if they have really big uncertainty on what's going to happen? Especially with something like the American presidential election like we've done a lot of presidential elections, we know that like usually all other things being equal, a decent guess is 50-50, and so if you have a lot of uncertainty that’s coming to you from your polls, it's going to… usually what these models do is if they have something called something like ‘reversion to the mean’ and they tend to just drift back toward 50%, which is basically saying like ‘we don't exactly know what's going on’ and like you know, ‘your guess is as good as mine, 50%, I don’t know.’ And then the other thing that is worth keeping in mind about the Brexit vote and some of the other betting sites that you see sometimes, this is another thing that I was reading up on after the Brexit vote. One of the reasons that people were so surprised by the Brexit vote is they actually have very… I was about to say ‘very healthy betting markets’, I don’t know if ‘healthy’ is totally a fair word to use, but you know, robust, active betting markets where you can actually go on and bet real money about what the outcome is going to be. And so since those are markets, they're supposedly subject to market facts that help them hypothetically be more accurate than polls maybe, depending on how you think about it, and you know, thinking exactly about how markets are supposed to have good information in this sort of a topic all in and of itself. But let me put it this way, that hypothetically there are reasons why you would think that a betting market might have odds that are closer to the true odds than various polls or models or whatever. And the thing about the betting markets was the betting markets are saying you know 90% chance that we're going to remain in the EU, and that was part of the reason also that people were so surprised with this as betting markets were way wrong. |
| Ben: | Yeah. |
| Katie: | And so there's been, you know, this is about an analysis that’s trying to understand what might have been going on and... you know I'm not enough of an expert to exactly say like, I feel like I understand the underlying mechanics of how some of these things are hypothetically supposed to work, but I also don't understand the details of like what kind of psychology of people drive people to participate in these kinds of British betting markets, and I imagine that a lot of times they might not be betting completely in terms of what they think the actual odds are, but it's kind of speculative or it's hedging or people are doing it for fun, or there's all kinds of like weird things that could be going on in betting markets. And so I won't speculate too much as to exactly the exact dynamic that drives these markets but the point that I was reading about and then I think is worth keeping in mind is that, like I said, a lot of times these markets will tend to err on the side of thinking that things are 50/50 or the things are going to stay roughly the same as they've always been in the past, they tend to be a little bit conservative. Not conservative in the sense of like ‘Democratic’, ‘Republican’, like conservative or liberal. I mean it in the sense of like conservative that they tend to favor slightly like things just kind of continuing on as they have in the past versus things really getting rocked in a big way, in the way that like a ‘leave’ vote was a really big thing. |
| Ben: | Oh I see. |
| Katie: | And so then it becomes this kind of like self-perpetuating systems where everybody just kind of thinks that there's this conventional wisdom like ‘of course we're never going to leave the EU’ and that gets incorporated into the kinds of like, that becomes potentially even just an assumption in the model, that we think that Britain really doesn't want to leave, and so we would have to see something very very strong it's coming to us out of the polls to convince us that assumption is wrong, but I don't know maybe that assumption, may be that assumption is not as strong as we think it is or not as strong as it as we put it into a model to be, and so if we see a poll that says 53% for leave, you know, we might say like whatever, that's just a fluctuation, I know this because we've always voted this way in the past, we're just going to keep doing that. And then when 53% actually shows up on Election Day, then it was wrong. And so this was kind of one of the other things that I think happened with the Brexit vote, and people should be aware when they're trying to understand how to interpret some of these forecasts as you know they might say something like, well, there's a 25% chance of the outcome happening this way and people are gonna be like ‘Yeah, but we've never had anything like that really happened, so like I don't think it's really gonna happen.” |
| Ben: | So human psychology heavily waiting that there's no precedent for this and so therefore it's unlikely. |
| Katie: | Yeah, and so I think that was in the same kind of… people especially on the Brexit Election because that was just such a strange… |
| Ben: | Yeah, that’s a big one, it’s not like we've never elected someone that’s crazy, or we’ve never like… it’s not something like that, it’s like this is a country leaving a consortium, that’s kind of big. And it’s not something that happens that could conceivably happen every 4 years, not necessarily either.
